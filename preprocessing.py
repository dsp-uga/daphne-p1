# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OCBjbsfNzp01lBTE6EacJFJpdG4xa4dm
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz
!tar xf spark-3.0.1-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()
spark

# import pyspark
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.sql import SQLContext

# create Spark context with Spark configuration
conf = SparkConf().setAppName("read text file")
sc = SparkContext.getOrCreate(conf=conf)

# Read files into RDD
lines_small_train_X = sc.textFile("data/project1_files_X_small_train.txt")
lines_small_train_Y = sc.textFile("project1_files_y_small_train.txt")


# Call collect() to get all data
list_small_train_X = lines_small_train_X.collect()
list_small_train_Y = lines_small_train_Y.collect()

# print line one by line
# for line in llist:
# 	print(line)

print(list_small_train_X)
print(list_small_train_Y)

# import pyspark
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.sql import SQLContext
from pyspark.sql.functions import split

# read text files into Spark dataframes
df_small_X = spark.read.load("project1_files_X_small_train.txt",
                 format="csv", sep=" ", inferSchema="true", header="false").toDF("X")
df_small_Y = spark.read.load("project1_files_y_small_train.txt",
                 format="csv", sep=" ", inferSchema="true", header="false").toDF("Y")
# combine X and Y into one dataframe
df_small = df_small_X.join(df_small_Y)
df_small.show()

# create Spark context with Spark configuration
conf = SparkConf().setAppName("read text file")
sc = SparkContext.getOrCreate(conf=conf)

# using hashes to locate the .asm file
path = 'hdfs://data/bytes'
# def ReadAsm(hash):
#   file = sc.textFile(os.path.join(path, ''.join([hash,'.asm'])))
#   asm_file = file.collect() 
#   return asm_file

# retrieves every character in a bytes file into a list
def ReadBytes(hash):
  file = sc.textFile(os.path.join(path, ''.join([hash,'.bytes'])))
  print(file)
  bytes_file = file.collect()
  characters = []
  for i in asm1:
    for j in i.split()[1:16]:
      characters.append(j) 
  most_frequent = Counter(characters).most_common(10)
  return characters

# tf-idf generating
from pyspark.mllib.feature import HashingTF, IDF

# Load documents (one per line).
documents = sc.textFile("01SuzwMJEIXsK7A8dQbl.bytes").map(lambda line: line.split(" "))

hashingTF = HashingTF()
tf = hashingTF.transform(documents)

# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:
# First to compute the IDF vector and second to scale the term frequencies by IDF.
tf.cache()
idf = IDF().fit(tf)
tfidf = idf.transform(tf)

# spark.mllib's IDF implementation provides an option for ignoring terms
# which occur in less than a minimum number of documents.
# In such cases, the IDF for these terms is set to 0.
# This feature can be used by passing the minDocFreq value to the IDF constructor.
idfIgnore = IDF(minDocFreq=2).fit(tf)
tfidfIgnore = idfIgnore.transform(tf)
tfidf.take(5)

path = 'data'
hash = 'project1_files_X_small_train'
root = os.path.join(path, ''.join([hash,'.txt']))
file = sc.textFile(root)
file1 = file.collect()

print(file1)

from collections import Counter
asm = sc.textFile("01SuzwMJEIXsK7A8dQbl.bytes")
asm1 = asm.collect()
characters = []
for i in asm1:
  for j in i.split()[1:16]:
    characters.append(j)

print(characters)
most_frequent = Counter(characters).most_common(10)
most_frequent