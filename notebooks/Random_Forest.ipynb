{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tet960DAc9bz"
      },
      "source": [
        "# install Spark\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\r\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuVD-D47h1OX"
      },
      "source": [
        "!pip install -q findspark\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTYyZ0J1h6qb"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\r\n",
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNVKTdMFYa0h"
      },
      "source": [
        "# parameters for random forest\r\n",
        "TRAIN_PATH = \"X_small_train.csv\"\r\n",
        "TEST_PATH = \"X_small_test.csv\"\r\n",
        "LABELS = \"project1_files_y_small_train.txt\"\r\n",
        "LABELS_TEST = \"y_small_test.txt\"\r\n",
        "APP_NAME = \"Random Forest Classifier\"\r\n",
        "SPARK_URL = \"local[*]\"\r\n",
        "IMPURITY = \"gini\"\r\n",
        "RANDOM_SEED = 13579\r\n",
        "RF_NUM_TREES = 3\r\n",
        "RF_MAX_DEPTH = 4\r\n",
        "RF_MAX_BINS = 32"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM2Kp9jXZsPD"
      },
      "source": [
        "from pyspark import SparkContext\r\n",
        "from pyspark.mllib.linalg import Vectors\r\n",
        "from pyspark.mllib.regression import LabeledPoint\r\n",
        "from pyspark.mllib.tree import RandomForest\r\n",
        "from time import *\r\n",
        "from pyspark.sql import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caNvFN6sZv_Y"
      },
      "source": [
        "\r\n",
        "# read csv features file into dataframe\r\n",
        "df_train_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TRAIN_PATH)\r\n",
        "\r\n",
        "# read txt labels file into dataframe\r\n",
        "df_train_Y = spark.read.load(LABELS, format=\"csv\", sep=\" \", inferSchema=\"true\", header=\"false\").toDF('Y')\r\n",
        "\r\n",
        "# change labels (1,2,...,9) to int (0,1,2,...,8), for the purpose of classification\r\n",
        "df_train_Y = df_train_Y.withColumn(\"Y\",col(\"Y\") - lit(1))\r\n",
        "\r\n",
        "# combine features and labels into one dataframe\r\n",
        "# first, create a row index list w\r\n",
        "w = Window.orderBy(lit(1))\r\n",
        "\r\n",
        "# add row indexs to dataframes X and Y\r\n",
        "df_X=df_train_X.withColumn(\"rn\",row_number().over(w)-1)\r\n",
        "df_Y=df_train_Y.withColumn(\"rn\",row_number().over(w)-1)\r\n",
        "\r\n",
        "# join X dataframe and Y dataframe; and drop the row index\r\n",
        "df_train = df_X.join(df_Y,[\"rn\"]).drop(\"rn\")\r\n",
        "\r\n",
        "### if one would like to view the dataframe, please uncomment below line.\r\n",
        "# df_train.show()\r\n",
        "\r\n",
        "### below two lines are abandoned. don't worry about them.\r\n",
        "# drop first two columns for training\r\n",
        "# df_train = df_train.drop('_c0','hash')\r\n",
        "\r\n",
        "\r\n",
        "# put training dataframe into a RDD of LabeledPoint.\r\n",
        "# column_0 is index, and column_hash is hashes; we don't use them for training.\r\n",
        "# the last column, 'Y', is our label column. \r\n",
        "transformed_df = df_train.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[2:-1])))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ1y2cloA236"
      },
      "source": [
        "splits = [0.9, 0.1]\r\n",
        "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\r\n",
        "\r\n",
        "print(\"Number of training set rows: %d\" % training_data.count())\r\n",
        "print(\"Number of test set rows: %d\" % test_data.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hN3C40BdSwn"
      },
      "source": [
        "# set up a start time, in the purpose of recording the length of training time.\r\n",
        "start_time = time()\r\n",
        "\r\n",
        "# model of randomforest\r\n",
        "model = RandomForest.trainClassifier(training_data, numClasses=9, categoricalFeaturesInfo={}, numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=IMPURITY, maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=RANDOM_SEED)\r\n",
        "\r\n",
        "### model parameters\r\n",
        "# categoricalFeaturesInfo - Map storing arity of categorical features. An entry (n to k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.\r\n",
        "# featureSubsetStrategy - Number of features to consider for splits at each node.\r\n",
        "# impurity - Criterion used for information gain calculation. See https://spark.apache.org/docs/latest/mllib-decision-tree.html for more details\r\n",
        "# maxDepth - Maximum depth of the tree (e.g. depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes).\r\n",
        "# maxBins - Maximum number of bins used for splitting features\r\n",
        "\r\n",
        "end_time = time()\r\n",
        "training_time = end_time - start_time\r\n",
        "print(\"Time to train model: %.3f seconds\" % training_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLsnK2CGe9cw"
      },
      "source": [
        "# prediction\r\n",
        "predictions = model.predict(test_data.map(lambda x: x.features))\r\n",
        "labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)\r\n",
        "acc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\r\n",
        "print(\"Model accuracy: %.3f%%\" % (acc * 100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}