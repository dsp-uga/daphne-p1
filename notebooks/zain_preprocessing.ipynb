{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, ArrayType\n",
    "from pyspark.sql.functions import collect_set\n",
    "from argparse import ArgumentParser\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, Tokenizer\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='gs://uga-dsp'\n",
    "minInitialCount=2\n",
    "bytesDir=f\"{bucket}/project1/data/bytes/\"\n",
    "asmDir=f\"{bucket}/project1/data/asm/\"\n",
    "filesDir=f\"{bucket}/project1/files/\"\n",
    "dest='gs://micky-practicum/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexGen(i):\n",
    "    return ('0'+str(hex(i)).upper()[2:])[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assembleSchema():\n",
    "    out=[hexGen(i) for i in range(256)]+['??']\\\n",
    "       +['rel_'+hexGen(i) for i in range(256)]+['rel_??']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema=sc.broadcast(_assembleSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSingleSchema():\n",
    "    schema = StructType([StructField('hash',StringType())]\\\n",
    "                        +[StructField(_schema.value[i],LongType()) for i in range(257)])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.wholeTextFiles(bytesDir,minPartitions=45)\n",
    "schema=StructType([StructField('file_name',StringType()),StructField('contents',StringType())])\n",
    "df=spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"contents\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=tokenized.drop('contents').rdd\n",
    "pattern=sc.broadcast('\\w+(?=\\.bytes)')\n",
    "rdd3=rdd2.map(lambda x: (re.findall(pattern.value, x[0])[0],[y for y in x[1] if len(y)==2]))\n",
    "\n",
    "schema2=StructType([StructField('hash',StringType()),StructField('words',ArrayType(StringType()))])\n",
    "tokenized2=spark.createDataFrame(rdd3,schema2)\n",
    "tokenized2.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"word_count\")\n",
    "\n",
    "model = cv.fit(tokenized2)\n",
    "\n",
    "result = model.transform(tokenized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "spark.catalog.clearCache()\n",
    "result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(\"hash\", \"word_count\").write.save(dest+\"X_train_pre.parquet\",mode='overwrite')\n",
    "print('finished')\n",
    "#result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = sqlContext.read.load(dest+'X_train_pre.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_to_index(code):\n",
    "    index=None\n",
    "    if code=='??':\n",
    "        index=256\n",
    "    else:\n",
    "        index=int(code[-2:],16)\n",
    "    print(f'{code}:{index}')\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimal_vocab=[code_to_index(code) for code in model.vocabulary]\n",
    "vocab_index=sc.broadcast([decimal_vocab.index(i) for i in range(len(decimal_vocab))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name='X_small_test'\n",
    "indicator_file=sc.broadcast(sc.textFile(f'{filesDir}{set_name}.txt').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted=intermediate.rdd.map(lambda x:list([x[0]]+[int(x[1][vocab_index.value[i]]) for i in range(257)]))\\\n",
    "            .filter(lambda x:x[0] in indicator_file.value)\n",
    "df_formatted=spark.createDataFrame(formatted,schema=buildSingleSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted.write.save(f\"{dest}counts/{set_name}.parquet\",mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
