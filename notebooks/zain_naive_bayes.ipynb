{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tet960DAc9bz"
   },
   "outputs": [],
   "source": [
    "# install Spark\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.0.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuVD-D47h1OX"
   },
   "outputs": [],
   "source": [
    "# !pip install -q findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTYyZ0J1h6qb"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNVKTdMFYa0h"
   },
   "outputs": [],
   "source": [
    "# parameters for random forest\n",
    "bucket=\"gs://micky-practicum/\"\n",
    "TRAIN_PATH = f\"{bucket}X_small_train.csv\"\n",
    "TEST_PATH = f\"{bucket}X_small_test.csv\"\n",
    "LABELS = f\"{bucket}y_small_train.txt\"\n",
    "LABELS_TEST = f\"{bucket}y_small_test.txt\"\n",
    "APP_NAME = \"Naive Bayes Classifier\"\n",
    "SPARK_URL = \"local[*]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM2Kp9jXZsPD"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import lit, row_number\n",
    "#from operator import add\n",
    "#from functools import reduce\n",
    "from math import log\n",
    "import re\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, FloatType, ArrayType\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SQLContext, Row, SparkSession, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"FirstNotebook\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caNvFN6sZv_Y"
   },
   "outputs": [],
   "source": [
    "# read csv features file into dataframe\n",
    "df_train_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TRAIN_PATH)\n",
    "\n",
    "# drop columns that are relitive values and the index column and \"hash\" column\n",
    "columns = []\n",
    "for name in df_train_X.schema.names:\n",
    "  if 'rel' in name:\n",
    "    columns.append(name)\n",
    "df_train_X = df_train_X.drop(*columns,'_c0','hash','total_count')\n",
    "\n",
    "# read txt labels file into dataframe\n",
    "df_train_Y = spark.read.load(LABELS, format=\"csv\", sep=\" \", inferSchema=\"true\", header=\"false\").toDF('Y')\n",
    "\n",
    "\n",
    "# combine features and labels into one dataframe\n",
    "# first, create a row index list w\n",
    "w = Window.orderBy(lit(1))\n",
    "\n",
    "# add row indexs to dataframes X and Y\n",
    "df_X=df_train_X.withColumn(\"rn\",row_number().over(w)-1)\n",
    "df_Y=df_train_Y.withColumn(\"rn\",row_number().over(w)-1)\n",
    "\n",
    "# join X dataframe and Y dataframe; and drop the row index\n",
    "df_train = df_X.join(df_Y,[\"rn\"]).drop(\"rn\")\n",
    "\n",
    "### if one would like to view the dataframe, please uncomment below line.\n",
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSchema(mode):\n",
    "    _schema=df_train.schema.fields\n",
    "    t=None\n",
    "    if mode=='int':\n",
    "        t=IntegerType\n",
    "    elif mode=='long':\n",
    "        t=LongType\n",
    "    elif mode=='float':\n",
    "        t=FloatType\n",
    "    schema = StructType([StructField('Y',LongType())]+[StructField(_schema[i].name,t()) for i in range(257)])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce df to save space and make computation easier\n",
    "df_train_reduced=spark.createDataFrame(df_train.groupBy('Y').sum().drop('sum(Y)').rdd.map(lambda x:tuple([x[0]]+[x[1+i]+1 for i in range(257)])),buildSchema('long'))\n",
    "\n",
    "# Cache for reuse\n",
    "#spark.catalog.clearCache()\n",
    "df_train_reduced.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to access total count of a single character across entire dataset\n",
    "def netCount(word):\n",
    "    return df_train_reduced.select(word).groupBy().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using netCount\n",
    "netCount('01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many instances of each class\n",
    "class_table=df_train.groupBy('Y').count().orderBy('Y')\n",
    "\n",
    "# Count how many instances there are total\n",
    "dataset_length=class_table.groupBy().sum().collect()[0][1]\n",
    "\n",
    "# Create prior distribution\n",
    "pc=spark.createDataFrame(class_table.rdd.map(lambda x:(x[0],x[1]/dataset_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ln(P(yk)) and collect into an array\n",
    "temp=pc.collect()\n",
    "class_probabilities=sc.broadcast([log(x[1]) for x in temp])\n",
    "del temp\n",
    "#print(class_probabilities.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word count dictionary to be broadcast and used to normalize columns\n",
    "\n",
    "wordCounts=sc.broadcast([netCount(word) for word in df_train.schema.names[:-1]])\n",
    "\n",
    "# Clear cache since netCount needn't be called anymore\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-wise normalized to reflect naive bayes formula\n",
    "train_log_weighted=df_train_reduced.rdd.map(lambda x:tuple([x[0]]+[log(x[i+1]/wordCounts.value[i]) for i in range(257)]))\n",
    "\n",
    "# Constructin dataframe off of the normalized RDD\n",
    "df_train_log_weighted=spark.createDataFrame(train_log_weighted,schema=buildSchema('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that contains the values corresponding to ln(P(x|y))\n",
    "_word_log_probabilities=df_train_log_weighted.orderBy('Y').drop('Y').collect()\n",
    "\n",
    "# Broadcast for distribution\n",
    "word_log_probabilities=sc.broadcast(_word_log_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeridATrm2Hr"
   },
   "outputs": [],
   "source": [
    "# read test features.csv into dataframe.\n",
    "df_test_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TEST_PATH)\n",
    "\n",
    "# drop columns that are relitive values and the index column and \"hash\" column\n",
    "columns = [name for name in df_test_X.schema.names if 'rel' in name]\n",
    "df_test_X = df_test_X.drop(*columns,'_c0','hash','total_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability of a particular word appearing, given a class\n",
    "# Note that @param word_index refers to an int value between [0,256]\n",
    "# Corresponding to the hex words [00,...,FF,??]\n",
    "def pWordGivenClass(word_index,c):\n",
    "    return word_log_probabilities.value[c][word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function factory that provides a lambda function for later in the pipeline\n",
    "# The function produced takes a document's wordcount and produces the top-k\n",
    "# Words by frequency\n",
    "def pClassGivenDocLambda(k):\n",
    "    def _pClassGivenDoc(doc):\n",
    "        _doc=np.array([doc[i] for i in range(len(doc))])\n",
    "        top_k=_doc.argsort()[::-1][:k].tolist()\n",
    "        likelihoods=[class_probabilities.value[i] for i in range(9)]\n",
    "        print(top_k)\n",
    "        for i in range(9):\n",
    "            for w in top_k:\n",
    "                prob=pWordGivenClass(w,i)\n",
    "                #print(f'Likelihood of class {i+1}|{w}({hexGen(w)})={prob}')\n",
    "                likelihoods[i]+=prob\n",
    "        return likelihoods\n",
    "    return _pClassGivenDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to get the two-letter hex-string corresponding to a word_index\n",
    "# @param i: int in [0,256] mapped to corresponding hex in [00,...,FF,??]\n",
    "def hexGen(i):\n",
    "    return ('0'+str(hex(i)).upper()[2:])[-2:] if i <256 else '??'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell (ignore)\n",
    "'''\n",
    "_test_top_k=df_test_X.rdd.map(pClassGivenDocLambda(4))\n",
    "_test_top_k.collect()[1]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the schema for the log-likelihood dataframe\n",
    "def buildLikelihoodSchema():\n",
    "    schema = StructType([StructField(f'{i+1}',FloatType()) for i in range(9)])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes class likelihoods and outputs the class which maximizes NLL\n",
    "def findMaxLikelihood(x):\n",
    "    row_as_list=list(x)\n",
    "    #print(row_as_list)\n",
    "    array=np.array(row_as_list)\n",
    "    #print(array)\n",
    "    max_arg=array.argmax()\n",
    "    #print(max_arg)\n",
    "    return [int(max_arg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe which stores the top k-many words per document\n",
    "k=50\n",
    "df_test_top_k=spark.createDataFrame(df_test_X.rdd.map(pClassGivenDocLambda(k)),schema=buildLikelihoodSchema())\n",
    "\n",
    "# Cast predictions \n",
    "df_predictions=spark.createDataFrame(df_test_top_k.rdd.map(findMaxLikelihood)).selectExpr(\"_1 as Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.coalesce(1).write.save(f\"{bucket}NB_small_test.csv\", format=\"csv\",mode='overwrite')\n",
    "\n",
    "#df_predictions.write.format(\"csv\").mode(\"overwrite\").save(f\"{bucket}NB_small_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
