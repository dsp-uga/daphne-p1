{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNVKTdMFYa0h"
   },
   "outputs": [],
   "source": [
    "bucket=\"gs://micky-practicum/\"\n",
    "indicator_path='gs://uga-dsp/project1/files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM2Kp9jXZsPD"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import re\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, FloatType, ArrayType, StringType\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train = sqlContext.read.load(f'{bucket}counts/X_train.parquet').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to get the two-letter hex-string corresponding to a word_index\n",
    "# @param i: int in [0,256] mapped to corresponding hex in [00,...,FF,??]\n",
    "def hexGen(i):\n",
    "    return ('0'+str(hex(i)).upper()[2:])[-2:] if i <256 else '??'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashes=sc.textFile(f'{indicator_path}X_train.txt').collect()\n",
    "train_labels=sc.textFile(f'{indicator_path}y_train.txt').collect()\n",
    "hash_labels=sc.broadcast({train_hashes[i]:train_labels[i] for i in range(len(train_hashes))})\n",
    "del train_hashes\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_to_label(x):\n",
    "    out=[int(hash_labels.value[x[0]])]\n",
    "    out+=x[1:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSchema(mode):\n",
    "    t=None\n",
    "    if mode=='int':\n",
    "        t=IntegerType\n",
    "    elif mode=='long':\n",
    "        t=LongType\n",
    "    elif mode=='float':\n",
    "        t=FloatType\n",
    "    schema = StructType([StructField('Y',LongType())]+[StructField(hexGen(i),t()) for i in range(257)])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with training data\n",
    "df_train=spark.createDataFrame(_train.map(hash_to_label),schema=buildSchema('long'))\n",
    "\n",
    "# Reduce df to save space and make computation easier\n",
    "df_train_reduced=spark.createDataFrame(df_train.groupBy('Y').sum().drop('sum(Y)').rdd.map(lambda x:tuple([x[0]]+[x[1+i]+1 for i in range(257)])),buildSchema('long'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many instances of each class\n",
    "class_table=df_train.groupBy('Y').count().orderBy('Y')\n",
    "\n",
    "# Count how many instances there are total\n",
    "dataset_length=class_table.groupBy().sum().collect()[0][1]\n",
    "\n",
    "# Create prior distribution\n",
    "pc=spark.createDataFrame(class_table.rdd.map(lambda x:(x[0],x[1]/dataset_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ln(P(yk)) and collect into an array\n",
    "temp=pc.collect()\n",
    "class_probabilities=sc.broadcast([log(x[1]) for x in temp])\n",
    "del temp\n",
    "#print(class_probabilities.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word count dictionary to be broadcast and used to normalize columns\n",
    "\n",
    "wordCounts=sc.broadcast(df_train_reduced.drop('Y').rdd.reduce(lambda x,y:[x[i]+y[i] for i in range(len(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-wise normalized to reflect naive bayes formula\n",
    "train_log_weighted=df_train_reduced.rdd.map(lambda x:tuple([x[0]]+[log(x[i+1]/wordCounts.value[i]) for i in range(257)]))\n",
    "\n",
    "# Constructin dataframe off of the normalized RDD\n",
    "df_train_log_weighted=spark.createDataFrame(train_log_weighted,schema=buildSchema('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that contains the values corresponding to ln(P(x|y))\n",
    "_word_log_probabilities=df_train_log_weighted.orderBy('Y').drop('Y').collect()\n",
    "\n",
    "# Broadcast for distribution\n",
    "word_log_probabilities=sc.broadcast(_word_log_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_X = sqlContext.read.load(f'{bucket}counts/X_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability of a particular word appearing, given a class\n",
    "# Note that @param word_index refers to an int value between [0,256]\n",
    "# Corresponding to the hex words [00,...,FF,??]\n",
    "def pWordGivenClass(word_index,c):\n",
    "    return word_log_probabilities.value[c][word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function factory that provides a lambda function for later in the pipeline\n",
    "# The function produced takes a document's wordcount and produces the top-k\n",
    "# Words by frequency\n",
    "def pClassGivenDoc(doc):\n",
    "    likelihoods=[class_probabilities.value[i] for i in range(9)]\n",
    "    for i in range(9):\n",
    "        for w in range(257):\n",
    "            prob=pWordGivenClass(w,i)\n",
    "            #print(f'Likelihood of class {i+1}|{w}({hexGen(w)})={prob}')\n",
    "            likelihoods[i]+=prob*doc[1+w]\n",
    "    return [doc[0]]+likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the schema for the log-likelihood dataframe\n",
    "def buildLikelihoodSchema():\n",
    "    schema = StructType([StructField('hash',StringType())]+[StructField(f'{i+1}',FloatType()) for i in range(9)])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes class likelihoods and outputs the class which maximizes NLL\n",
    "def findMaxLikelihood(x):\n",
    "    row_as_list=list(x[1:])\n",
    "    #print(row_as_list)\n",
    "    array=np.array(row_as_list)\n",
    "    #print(array)\n",
    "    max_arg=array.argmax()\n",
    "    #print(max_arg)\n",
    "    return [x[0]]+[int(max_arg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe which stores the top k-many words per document\n",
    "df_test_top_k=spark.createDataFrame(df_test_X.rdd.map(pClassGivenDoc),schema=buildLikelihoodSchema())\n",
    "\n",
    "# Cast predictions \n",
    "df_predictions=spark.createDataFrame(df_test_top_k.rdd.map(findMaxLikelihood),schema=StructType([StructField('hash',StringType()),StructField('class',IntegerType())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=df_predictions.rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hashes=sc.textFile(f'{indicator_path}X_test.txt').collect()\n",
    "with open('prediction.txt', 'a') as the_file:\n",
    "    for h in test_hashes:\n",
    "        the_file.write(f'{predictions[h]}\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
