{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tet960DAc9bz"
      },
      "source": [
        "# install Spark\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\r\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuVD-D47h1OX"
      },
      "source": [
        "!pip install -q findspark\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTYyZ0J1h6qb"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\r\n",
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNVKTdMFYa0h"
      },
      "source": [
        "# parameters for random forest\r\n",
        "TRAIN_PATH = \"X_small_train.csv\"\r\n",
        "TEST_PATH = \"X_small_test.csv\"\r\n",
        "LABELS = \"y_small_train.txt\"\r\n",
        "LABELS_TEST = \"y_small_test.txt\"\r\n",
        "APP_NAME = \"Naive Bayes Classifier\"\r\n",
        "SPARK_URL = \"local[*]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM2Kp9jXZsPD"
      },
      "source": [
        "from pyspark import SparkContext\r\n",
        "from pyspark.mllib.linalg import Vectors\r\n",
        "from pyspark.mllib.regression import LabeledPoint\r\n",
        "from pyspark.mllib.tree import RandomForest\r\n",
        "from time import *\r\n",
        "from pyspark.sql import *\r\n",
        "# from pyspark.sql.functions import *\r\n",
        "from operator import add\r\n",
        "\r\n",
        "\r\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caNvFN6sZv_Y"
      },
      "source": [
        "# read csv features file into dataframe\r\n",
        "df_train_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TRAIN_PATH)\r\n",
        "\r\n",
        "# drop columns that are relitive values and the index column and \"hash\" column\r\n",
        "columns = []\r\n",
        "for name in df_train_X.schema.names:\r\n",
        "  if 'rel' in name:\r\n",
        "    columns.append(name)\r\n",
        "df_train_X = df_train_X.drop(*columns,'_c0','hash','total_count')\r\n",
        "\r\n",
        "# read txt labels file into dataframe\r\n",
        "df_train_Y = spark.read.load(LABELS, format=\"csv\", sep=\" \", inferSchema=\"true\", header=\"false\").toDF('Y')\r\n",
        "\r\n",
        "\r\n",
        "# combine features and labels into one dataframe\r\n",
        "# first, create a row index list w\r\n",
        "w = Window.orderBy(lit(1))\r\n",
        "\r\n",
        "# add row indexs to dataframes X and Y\r\n",
        "df_X=df_train_X.withColumn(\"rn\",row_number().over(w)-1)\r\n",
        "df_Y=df_train_Y.withColumn(\"rn\",row_number().over(w)-1)\r\n",
        "\r\n",
        "# join X dataframe and Y dataframe; and drop the row index\r\n",
        "df_train = df_X.join(df_Y,[\"rn\"]).drop(\"rn\")\r\n",
        "\r\n",
        "### if one would like to view the dataframe, please uncomment below line.\r\n",
        "# df_train.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmdqGhkvWHQo"
      },
      "source": [
        "# split dataframes by the labels (Y=1, 2, 3, ..., 9)\r\n",
        "df_train.write.partitionBy(\"Y\").saveAsTable(\"dataframes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7FC2ptUWqTm"
      },
      "source": [
        "# read splitted dataframes for each class (from 1 to 9)\r\n",
        "df1 = spark.read.parquet(\"spark-warehouse/dataframes/Y=1/*.parquet\")\r\n",
        "df2 = spark.read.parquet(\"spark-warehouse/dataframes/Y=2/*.parquet\")\r\n",
        "df3 = spark.read.parquet(\"spark-warehouse/dataframes/Y=3/*.parquet\")\r\n",
        "df4 = spark.read.parquet(\"spark-warehouse/dataframes/Y=4/*.parquet\")\r\n",
        "df5 = spark.read.parquet(\"spark-warehouse/dataframes/Y=5/*.parquet\")\r\n",
        "df6 = spark.read.parquet(\"spark-warehouse/dataframes/Y=6/*.parquet\")\r\n",
        "df7 = spark.read.parquet(\"spark-warehouse/dataframes/Y=7/*.parquet\")\r\n",
        "df8 = spark.read.parquet(\"spark-warehouse/dataframes/Y=8/*.parquet\")\r\n",
        "df9 = spark.read.parquet(\"spark-warehouse/dataframes/Y=9/*.parquet\")\r\n",
        "\r\n",
        "# calculate P(yk).\r\n",
        "pc1 = df1.count()/df_train.count()\r\n",
        "pc2 = df2.count()/df_train.count()\r\n",
        "pc3 = df3.count()/df_train.count()\r\n",
        "pc4 = df4.count()/df_train.count()\r\n",
        "pc5 = df5.count()/df_train.count()\r\n",
        "pc6 = df6.count()/df_train.count()\r\n",
        "pc7 = df7.count()/df_train.count()\r\n",
        "pc8 = df8.count()/df_train.count()\r\n",
        "pc9 = df9.count()/df_train.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l2GE8orauLR"
      },
      "source": [
        "+# creates empty lists for storing conditional probabilities for each class. (e.g. p1 for class 1)\r\n",
        "# each list has 257 elements, relating to 257 words in \"vocabulary\", in the order of: 00,01,02,...,??\r\n",
        "class1 = []\r\n",
        "class2 = []\r\n",
        "class3 = []\r\n",
        "class4 = []\r\n",
        "class5 = []\r\n",
        "class6 = []\r\n",
        "class7 = []\r\n",
        "class8 = []\r\n",
        "class9 = []\r\n",
        "\r\n",
        "# iteration from \"00\" to \"??\", totoally 257 interations\r\n",
        "for word in df1.schema.names:\r\n",
        "  # calculate total count of one word among all documents\r\n",
        "  total = df_train.agg(sum(word)).collect()[0][0]\r\n",
        "\r\n",
        "  # append probabilities to each class. count of each word in the documents belong to the specific class, divided by total count of this word among all documents.\r\n",
        "  class1.append(df1.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class2.append(df2.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class3.append(df3.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class4.append(df4.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class5.append(df5.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class6.append(df6.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class7.append(df7.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class8.append(df8.agg(sum(word)).collect()[0][0]/total)\r\n",
        "  class9.append(df9.agg(sum(word)).collect()[0][0]/total)\r\n",
        "\r\n",
        "# should run around 8 minutes on Google Colab."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeridATrm2Hr"
      },
      "source": [
        "# read test features.csv into dataframe.\r\n",
        "df_test_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TEST_PATH)\r\n",
        "\r\n",
        "# drop columns that are relitive values and the index column and \"hash\" column\r\n",
        "columns = []\r\n",
        "for name in df_test_X.schema.names:\r\n",
        "  if 'rel' in name:\r\n",
        "    columns.append(name)\r\n",
        "df_test_X = df_test_X.drop(*columns,'_c0','hash','total_count')\r\n",
        "\r\n",
        "# empty list to store predicted labels\r\n",
        "labels_predicted = []\r\n",
        "\r\n",
        "# iterations for each row of testing dataset (# of testing data)\r\n",
        "for r in range(df_test_X.count()):\r\n",
        "\r\n",
        "  # create an empty list to store conditional probabilities for all 9 classes.\r\n",
        "  prob = []\r\n",
        "\r\n",
        "  # iteration for each word (257 in total)\r\n",
        "  for c in range(len(df1.columns)):\r\n",
        "    # calculate probabilty of each class and append to prob list.\r\n",
        "    prob.append(class1[c]**df_test_X.collect()[r][c]*pc1)\r\n",
        "    prob.append(class2[c]**df_test_X.collect()[r][c]*pc2)\r\n",
        "    prob.append(class3[c]**df_test_X.collect()[r][c]*pc3)\r\n",
        "    prob.append(class4[c]**df_test_X.collect()[r][c]*pc4)\r\n",
        "    prob.append(class5[c]**df_test_X.collect()[r][c]*pc5)\r\n",
        "    prob.append(class6[c]**df_test_X.collect()[r][c]*pc6)\r\n",
        "    prob.append(class7[c]**df_test_X.collect()[r][c]*pc7)\r\n",
        "    prob.append(class8[c]**df_test_X.collect()[r][c]*pc8)\r\n",
        "    prob.append(class9[c]**df_test_X.collect()[r][c]*pc9)\r\n",
        "  \r\n",
        "  # get the class with max probability\r\n",
        "  maximum = 0\r\n",
        "  for p in prob:\r\n",
        "    if p >= maximum:\r\n",
        "      maximum = p\r\n",
        "\r\n",
        "  # append predicted class to a list\r\n",
        "  labels_predicted.append(prob.index(maximum)+1)\r\n",
        "  \r\n",
        "# write predicted labels into txt file from the list we finished from above line\r\n",
        "with open(\"files.txt\", \"w\") as output:\r\n",
        "  for row in range(df_test_X.count()-1):\r\n",
        "    output.write(str(int(labels_predicted[row])) + '\\n')\r\n",
        "  output.write(str(int(labels_predicted[row])))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}