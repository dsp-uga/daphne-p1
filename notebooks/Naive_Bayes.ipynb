{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tet960DAc9bz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-3.0.1-bin-hadoop2.7.tgz'\n"
     ]
    }
   ],
   "source": [
    "# Shihan's code used as a starting point for spark & hadoop setup\n",
    "# install Spark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.0.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuVD-D47h1OX"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CTYyZ0J1h6qb"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8cbc04b9fa09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         raise Exception(\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         )\n\u001b[0;32m    148\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNVKTdMFYa0h"
   },
   "outputs": [],
   "source": [
    "# parameters for naive bayes\n",
    "TRAIN_PATH = \"X_small_train.csv\"\n",
    "TEST_PATH = \"X_small_test.csv\"\n",
    "LABELS = \"project1_files_y_small_train.txt\"\n",
    "LABELS_TEST = \"y_small_test.txt\"\n",
    "APP_NAME = \"Naive Bayes Classifier\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "IMPURITY = \"gini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM2Kp9jXZsPD"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.mllib.regression import LabeledPoint\n",
    "from time import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caNvFN6sZv_Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "# read csv features file into dataframe\n",
    "df_train_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TRAIN_PATH)\n",
    "\n",
    "# read txt labels file into dataframe\n",
    "df_train_Y = spark.read.load(LABELS, format=\"csv\", sep=\" \", inferSchema=\"true\", header=\"false\").toDF('Y')\n",
    "\n",
    "# change labels (1,2,...,9) to int (0,1,2,...,8), for the purpose of classification\n",
    "df_train_Y = df_train_Y.withColumn(\"Y\",col(\"Y\") - lit(1))\n",
    "\n",
    "# combine features and labels into one dataframe\n",
    "# first, create a row index list w\n",
    "w = Window.orderBy(lit(1))\n",
    "\n",
    "# add row indexs to dataframes X and Y\n",
    "df_X=df_train_X.withColumn(\"rn\",row_number().over(w)-1)\n",
    "df_Y=df_train_Y.withColumn(\"rn\",row_number().over(w)-1)\n",
    "\n",
    "# join X dataframe and Y dataframe; and drop the row index\n",
    "df_train = df_X.join(df_Y,[\"rn\"]).drop(\"rn\")\n",
    "\n",
    "### if one would like to view the dataframe, please uncomment below line.\n",
    "# df_train.show()\n",
    "\n",
    "### below two lines are abandoned. don't worry about them.\n",
    "# drop first two columns for training\n",
    "# df_train = df_train.drop('_c0','hash')\n",
    "\n",
    "\n",
    "# put training dataframe into a RDD of LabeledPoint.\n",
    "# column_0 is index, and column_hash is hashes; we don't use them for training.\n",
    "# the last column, 'Y', is our label column. \n",
    "transformed_df = df_train.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[2:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ1y2cloA236"
   },
   "outputs": [],
   "source": [
    "splits = [0.9, 0.1]\n",
    "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\n",
    "\n",
    "print(\"Number of training set rows: %d\" % training_data.count())\n",
    "print(\"Number of test set rows: %d\" % test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hN3C40BdSwn"
   },
   "outputs": [],
   "source": [
    "# set up a start time, in the purpose of recording the length of training time.\n",
    "start_time = time()\n",
    "\n",
    "# model of randomforest\n",
    "#model = RandomForest.trainClassifier(training_data, numClasses=9, categoricalFeaturesInfo={}, numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=IMPURITY, maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=RANDOM_SEED)\n",
    "\n",
    "### model parameters\n",
    "# categoricalFeaturesInfo - Map storing arity of categorical features. An entry (n to k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.\n",
    "# featureSubsetStrategy - Number of features to consider for splits at each node.\n",
    "# impurity - Criterion used for information gain calculation. See https://spark.apache.org/docs/latest/mllib-decision-tree.html for more details\n",
    "# maxDepth - Maximum depth of the tree (e.g. depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes).\n",
    "# maxBins - Maximum number of bins used for splitting features\n",
    "\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLsnK2CGe9cw"
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "# predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "# labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)\n",
    "# acc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "# print(\"Model accuracy: %.3f%%\" % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Random Forest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
