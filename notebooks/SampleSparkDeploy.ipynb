{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, LongType, FloatType\n",
    "from argparse import ArgumentParser\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parser = ArgumentParser(description='PySpark Data Processor')\n",
    "parser.add_argument('--minInitialCount', type=int, default=2, metavar='M',\n",
    "                    help='The minimum number of times a token must appear in a document\\\n",
    "                    to be considered for the global calculation')\n",
    "parser.add_argument('--byteDir', type=str, default='.', metavar='bD',\n",
    "                    help='The directory from which to pull binaries')\n",
    "parser.add_argument('--asmDir', type=str, default='.', metavar='aD',\n",
    "                    help='The directory from which to pull .asm files')\n",
    "parser.add_argument('--dest', type=str, default='.', metavar='D',\n",
    "                    help='The directory in which to store any output files')\n",
    "'''\n",
    "bucket='gs://uga-dsp'\n",
    "minInitialCount=2\n",
    "bytesDir=f\"{bucket}/project1/data/bytes/\"\n",
    "asmDir=f\"{bucket}/project1/data/asm/\"\n",
    "filesDir=f\"{bucket}/project1/files/\"\n",
    "dest='/home/zainmeekail/daphne-p1/features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"FirstNotebook\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines parsing steps given a .txt file which specifies the hashes\n",
    "of binary/asm files to include. \n",
    "\n",
    "NOTE: Currently the function will only deal with binaries, reflecting\n",
    "the approach decided on by the group. This may later be modified to\n",
    "accommodate asm files as well as binaries, or asm processing may occur\n",
    "in a seperate but similar function\n",
    "'''\n",
    "def datasetParser(file):\n",
    "    out=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.split())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines the main loop for computing features over a dataset\n",
    "@param dataset: the path to the dataset indicator function (abslute path preferred)\n",
    "@param func: the main processing function which can be specified per `main` call\n",
    "@param outName: the name of the output file, given as a path (abslute path preferred)\n",
    "                -- if outName is not defined, then the output will not be saved, only returned\n",
    "'''\n",
    "def main(dataset,func, outName=None):\n",
    "\n",
    "    # Declare df as a variable for later use\n",
    "    df=None\n",
    "    \n",
    "    # Construct schema\n",
    "    schema=buildSchema()\n",
    "    \n",
    "    # Initialize count for progress tracking\n",
    "    count=1\n",
    "    \n",
    "    # Collects the hash-list provided by the indicator file\n",
    "    dataList=datasetParser(filesDir+dataset).collect()\n",
    "\n",
    "    # Set a basic loop over the data to process all relevant files\n",
    "    for filename in dataList:\n",
    "        print(f'Processing hash: {filename}\\nProgress: {count}/{len(dataList)} | {count/len(dataList)*100:.2f}%')\n",
    "        \n",
    "        # Processes a single into a row entry using `func`\n",
    "        row=func(filename)\n",
    "        count+=1\n",
    "        \n",
    "        # First row creates the dataframe, the subsequent ones add to it\n",
    "        if df==None:\n",
    "            df=spark.createDataFrame([tuple(row)],schema=schema)\n",
    "        else:\n",
    "            df=df.union(spark.createDataFrame([tuple(row)],schema=schema))\n",
    "    \n",
    "    # Save dataframe as a csv if outName is defined\n",
    "    if outName is not None:\n",
    "        df.toPandas().to_csv(outName)\n",
    "\n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRow(file_hash):   \n",
    "    \n",
    "    row=[file_hash]\n",
    "    file=bytesDir+file_hash+'.bytes'\n",
    "    wc=wordCount(file)\n",
    "    X=wc.collectAsMap()\n",
    "    \n",
    "    row+=[safeCheck(X,hexGen(i)) for i in range(256)]+[safeCheck(X,'??')]\n",
    "    del X\n",
    "    \n",
    "    _net=wc.map(lambda x:x[1])\n",
    "    net=_net.reduce(lambda x,y:x+y)\n",
    "    _rel=wc.map(lambda x:('rel_'+x[0],x[1]/net))\n",
    "\n",
    "    X=_rel.collectAsMap()\n",
    "    row+=[safeCheck(X,'rel_'+hexGen(i),.0) for i in range(256)]+[safeCheck(X,'rel_??',.0)] \n",
    "    del X\n",
    "    \n",
    "    row+=[net]\n",
    "    return row\n",
    "#relative=temp.map(lambda x:x/net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines ops for a single document. This function handles all the operations\n",
    "which are fully contained within a single document (e.g. word count but not IDF). \n",
    "This function specifically generates a word-count for the document.\n",
    "\n",
    "'''\n",
    "\n",
    "def wordCount(file):\n",
    "    #print(\"Reading file: \"+file)\n",
    "    out=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.split())\\\n",
    "        .filter(lambda x: len(x)==2)\\\n",
    "        .map(lambda x:(x,1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeCheck(X,key,val=0):\n",
    "    return X[key] if key in X else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexGen(i):\n",
    "    return ('0'+str(hex(i)).upper()[2:])[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assembleSchema():\n",
    "    out=[hexGen(i) for i in range(256)]+['??']\\\n",
    "       +['rel_'+hexGen(i) for i in range(256)]+['rel_??']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema=sc.broadcast(_assembleSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSchema():\n",
    "    schema = StructType([StructField('hash',StringType())]\\\n",
    "                        +[StructField(_schema.value[i],LongType()) for i in range(257)]\\\n",
    "                        +[StructField(_schema.value[i],FloatType()) for i in range(257,514)]\\\n",
    "                        +[StructField('total_count',LongType())])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=main(\"X_small_train.txt\",buildRow,outName=dest+'X_small_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv(dest+'X_small_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=buildRow('DvdM5Zpx96qKuN3cAt1y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=buildSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note To Self:\n",
    "\n",
    "Must account for additional index column that is added during csv writing. Potentially use 'index=False' on writing, though unknown if this is supported in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load('gs://micky-practicum/'+'X_small_train.csv',schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DF=spark.read.csv('gs://micky-practicum/'+'X_small_train.csv',schema=schema,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('total_count').head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
