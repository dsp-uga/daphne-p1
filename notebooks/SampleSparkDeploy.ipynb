{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, ArrayType\n",
    "from pyspark.sql.functions import collect_set\n",
    "from argparse import ArgumentParser\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, Tokenizer\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parser = ArgumentParser(description='PySpark Data Processor')\n",
    "parser.add_argument('--minInitialCount', type=int, default=2, metavar='M',\n",
    "                    help='The minimum number of times a token must appear in a document\\\n",
    "                    to be considered for the global calculation')\n",
    "parser.add_argument('--byteDir', type=str, default='.', metavar='bD',\n",
    "                    help='The directory from which to pull binaries')\n",
    "parser.add_argument('--asmDir', type=str, default='.', metavar='aD',\n",
    "                    help='The directory from which to pull .asm files')\n",
    "parser.add_argument('--dest', type=str, default='.', metavar='D',\n",
    "                    help='The directory in which to store any output files')\n",
    "'''\n",
    "bucket='gs://uga-dsp'\n",
    "minInitialCount=2\n",
    "bytesDir=f\"{bucket}/project1/data/bytes/\"\n",
    "asmDir=f\"{bucket}/project1/data/asm/\"\n",
    "filesDir=f\"{bucket}/project1/files/\"\n",
    "dest='gs://micky-practicum/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'gs://dataproc-temp-us-east1-492533985610-4ibxr4au/56aac939-8c8b-47a6-9e3c-0ba7f1f63534/spark-job-history'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://pyspark-m:8088/proxy/application_1613711227585_0005'),\n",
       " ('spark.app.id', 'application_1613711227585_0005'),\n",
       " ('spark.sql.warehouse.dir', 'file:/spark-warehouse'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.driver.port', '33225'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.historyServer.address', 'pyspark-m:18080'),\n",
       " ('spark.yarn.unmanagedAM.enabled', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.app.startTime', '1613712872503'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://pyspark-m.us-east1-b.c.citric-passage-230819.internal:33261'),\n",
       " ('spark.sql.cbo.joinReorder.enabled', 'true'),\n",
       " ('spark.driver.maxResultSize', '1920m'),\n",
       " ('spark.executor.memory', '10346m'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.sql.adaptive.enabled', 'true'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10000'),\n",
       " ('spark.driver.host',\n",
       "  'pyspark-m.us-east1-b.c.citric-passage-230819.internal'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.eventLog.dir',\n",
       "  'gs://dataproc-temp-us-east1-492533985610-4ibxr4au/56aac939-8c8b-47a6-9e3c-0ba7f1f63534/spark-job-history'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'pyspark-m'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.driver.memory', '3840m'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '77m'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.cbo.enabled', 'true')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"DataProcessing\")\\\n",
    "    .set('spark.executor.instances', '5')\\\n",
    "    .set('spark.executor.cores', '4')\\\n",
    "    .set('spark.default.parallelism','60')\n",
    "print(conf.get('spark.default.parallelism'))\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "print(sc.defaultParallelism)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines parsing steps given a .txt file which specifies the hashes\n",
    "of binary/asm files to include. \n",
    "\n",
    "NOTE: Currently the function will only deal with binaries, reflecting\n",
    "the approach decided on by the group. This may later be modified to\n",
    "accommodate asm files as well as binaries, or asm processing may occur\n",
    "in a seperate but similar function\n",
    "'''\n",
    "def datasetParser(file):\n",
    "    out=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.split())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines the main loop for computing features over a dataset\n",
    "@param dataset: the path to the dataset indicator function (abslute path preferred)\n",
    "@param func: the main processing function which can be specified per `main` call\n",
    "@param outName: the name of the output file, given as a path (abslute path preferred)\n",
    "                -- if outName is not defined, then the output will not be saved, only returned\n",
    "'''\n",
    "def main(dataset,func, schema=None,outName=None ):\n",
    "\n",
    "    # Declare df as a variable for later use\n",
    "    df=None\n",
    "        \n",
    "    # Initialize count for progress tracking\n",
    "    count=0\n",
    "    \n",
    "    # Collects the hash-list provided by the indicator file\n",
    "    dataList=datasetParser(filesDir+dataset).collect()\n",
    "\n",
    "    # Set a basic loop over the data to process all relevant files\n",
    "    for filename in dataList:\n",
    "        print(f'Processing hash: {filename}\\nProgress: {count+1}/{len(dataList)} | {(count+1)/len(dataList)*100:.2f}%')\n",
    "        \n",
    "        # Processes a single into a row entry using `func`\n",
    "        row=func(filename)\n",
    "        count+=1\n",
    "        \n",
    "        # First row creates the dataframe, the subsequent ones add to it\n",
    "        if df==None:\n",
    "            df=spark.createDataFrame(row,schema=schema)\n",
    "        else:\n",
    "            df=df.union(spark.createDataFrame(row,schema=schema))\n",
    "   \n",
    "        spark.catalog.clearCache()\n",
    "        df.cache()\n",
    "    \n",
    "    # Save dataframe as a csv if outName is defined\n",
    "    if outName is not None:\n",
    "        df.toPandas().to_csv(outName,index=False)\n",
    "\n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colToIndex(x):\n",
    "    index=None\n",
    "    s=x[0]\n",
    "    val=x[1]\n",
    "    code=s[-2:]\n",
    "    if code=='??':\n",
    "        index=256\n",
    "    else:\n",
    "        index=int(s[-2:],16)\n",
    "    out=np.zeros(257)\n",
    "    out[index]=val\n",
    "    return out.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colToArray(x):\n",
    "    index=None\n",
    "    s=x[0]\n",
    "    val=x[1]\n",
    "    code=s[-2:]\n",
    "    if code=='??':\n",
    "        index=256\n",
    "    else:\n",
    "        index=int(s[-2:],16)\n",
    "    out=np.zeros(257)\n",
    "    out[index]=val\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSingleRow(file_hash):\n",
    "    row=[file_hash]\n",
    "    file=bytesDir+file_hash+'.bytes'\n",
    "    wc=wordCount(file)\n",
    "    X=wc.map(colToIndex).reduce(lambda x,y:x+y)\n",
    "    return [row+X.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRowFaster(file_hash):\n",
    "    row=[file_hash]\n",
    "    file=bytesDir+file_hash+'.bytes'\n",
    "    wc=wordCount(file)\n",
    "    X=wc.map(colToIndex).reduce(lambda x,y:x+y)\n",
    "    row+=X.astype(np.int32).tolist()\n",
    "    del X\n",
    "    \n",
    "    _net=wc.map(lambda x:x[1])\n",
    "    net=_net.reduce(lambda x,y:x+y)\n",
    "    _rel=wc.map(lambda x:('rel_'+x[0],x[1]/net))\n",
    "    X=_rel.map(colToIndex).reduce(lambda x,y:x+y)\n",
    "    row+=X.tolist()\n",
    "    del X\n",
    "    \n",
    "    row+=[net]\n",
    "    return [tuple(row)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRow(file_hash):   \n",
    "    \n",
    "    row=[file_hash]\n",
    "    file=bytesDir+file_hash+'.bytes'\n",
    "    wc=wordCount(file)\n",
    "    X=wc.collectAsMap()\n",
    "    \n",
    "    row+=[safeCheck(X,hexGen(i)) for i in range(256)]+[safeCheck(X,'??')]\n",
    "    del X\n",
    "    \n",
    "    _net=wc.map(lambda x:x[1])\n",
    "    net=_net.reduce(lambda x,y:x+y)\n",
    "    _rel=wc.map(lambda x:('rel_'+x[0],x[1]/net))\n",
    "\n",
    "    X=_rel.collectAsMap()\n",
    "    row+=[safeCheck(X,'rel_'+hexGen(i),.0) for i in range(256)]+[safeCheck(X,'rel_??',.0)] \n",
    "    del X\n",
    "    \n",
    "    row+=[net]\n",
    "    return [tuple(row)]\n",
    "#relative=temp.map(lambda x:x/net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines ops for a single document. This function handles all the operations\n",
    "which are fully contained within a single document (e.g. word count but not IDF). \n",
    "This function specifically generates a word-count for the document.\n",
    "\n",
    "'''\n",
    "\n",
    "def wordCount(file):\n",
    "    #print(\"Reading file: \"+file)\n",
    "    out=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.split())\\\n",
    "        .filter(lambda x: len(x)==2)\\\n",
    "        .map(lambda x:(x,1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeCheck(X,key,val=0):\n",
    "    return X[key] if key in X else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexGen(i):\n",
    "    return ('0'+str(hex(i)).upper()[2:])[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assembleSchema():\n",
    "    out=[hexGen(i) for i in range(256)]+['??']\\\n",
    "       +['rel_'+hexGen(i) for i in range(256)]+['rel_??']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema=sc.broadcast(_assembleSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSingleSchema():\n",
    "    schema = StructType([StructField('hash',StringType())]\\\n",
    "                        +[StructField(_schema.value[i],LongType()) for i in range(257)])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSchema():\n",
    "    schema = StructType([StructField('hash',StringType())]\\\n",
    "                        +[StructField(_schema.value[i],LongType()) for i in range(257)]\\\n",
    "                        +[StructField(_schema.value[i],FloatType()) for i in range(257,514)]\\\n",
    "                        +[StructField('total_count',LongType())])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.wholeTextFiles(bytesDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([StructField('file_name',StringType()),StructField('contents',StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"contents\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=tokenized.drop('contents').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2=StructType([StructField('hash',StringType()),StructField('words',ArrayType(StringType()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd2.map(lambda x: (re.findall(pattern.value, x[0])[0],[y for y in x[1] if len(y)==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized2=spark.createDataFrame(rdd3,schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=sc.broadcast('\\w+(?=\\.bytes)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"word_count\")\n",
    "\n",
    "model = cv.fit(tokenized2)\n",
    "\n",
    "result = model.transform(tokenized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "spark.catalog.clearCache()\n",
    "result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(\"hash\", \"word_count\").write.save(dest+\"X_train_pre.parquet\")\n",
    "print('finished')\n",
    "#result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df=main(\"X_small_train.txt\",buildSingleRow,buildSingleSchema(),outName=dest+'counts/X_small_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_hash='HV0ctLUKfW1ozkmC7BMJ'\n",
    "file=bytesDir+file_hash+'.bytes'\n",
    "#wc=wordCount(file)\n",
    "wc=wordCount(file).map(colToIndex).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBiGramRow(file_hash):\n",
    "    file=bytesDir+file_hash+'.bytes'\n",
    "    \n",
    "    rdd=sc.textFile(file).map(lambda x:{'line':x.split()[1:]})\n",
    "    docf=spark.createDataFrame(rdd)\n",
    "    \n",
    "    ngram = NGram(n=2,inputCol='line',outputCol='ngrams')\n",
    "    docf=ngram.transform(docf)\n",
    "    \n",
    "    cv = CountVectorizer(inputCol=\"ngrams\", outputCol=\"ngram count\")\n",
    "    model=cv.fit(docf)\n",
    "    result=model.transform(docf)\n",
    "    \n",
    "    _vocab=sc.broadcast(model.vocabulary)\n",
    "    \n",
    "    nc=result.select('ngram count').rdd\n",
    "    t=nc.flatMap(lambda x: ((int(i),int(x[0][int(i)])) for i in x[0].indices))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .map(lambda x:(_vocab.value[x[0]],x[1]))\n",
    "    \n",
    "    out=t.collect()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df=main(\"X_small_train.txt\",buildBiGramRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse=SparseVector(256*256,t.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=t.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramDict={model.vocabulary[x[0]]:x[1] for x in T}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = sqlContext.read.load(dest+'X_small_train.csv', \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true', \n",
    "                      inferSchema='true')\n",
    "\n",
    "# Alternative read method bellow (though you must discard first row due to inproper header loading)\n",
    "# DF = spark.read.format(\"csv\").load('gs://micky-practicum/'+'X_small_train.csv',schema=schema,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
