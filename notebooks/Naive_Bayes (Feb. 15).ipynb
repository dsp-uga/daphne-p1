{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Tet960DAc9bz"
   },
   "outputs": [],
   "source": [
    "# install Spark\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.0.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xuVD-D47h1OX"
   },
   "outputs": [],
   "source": [
    "# !pip install -q findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CTYyZ0J1h6qb"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JNVKTdMFYa0h"
   },
   "outputs": [],
   "source": [
    "# parameters for random forest\n",
    "TRAIN_PATH = \"X_small_train.csv\"\n",
    "TEST_PATH = \"X_small_test.csv\"\n",
    "LABELS = \"y_small_train.txt\"\n",
    "LABELS_TEST = \"y_small_test.txt\"\n",
    "APP_NAME = \"Naive Bayes Classifier\"\n",
    "SPARK_URL = \"local[*]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OM2Kp9jXZsPD"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from time import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caNvFN6sZv_Y"
   },
   "outputs": [],
   "source": [
    "# read csv features file into dataframe\n",
    "df_train_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TRAIN_PATH)\n",
    "\n",
    "# drop columns that are relitive values and the index column and \"hash\" column\n",
    "columns = []\n",
    "for name in df_train_X.schema.names:\n",
    "  if 'rel' in name:\n",
    "    columns.append(name)\n",
    "df_train_X = df_train_X.drop(*columns,'_c0','hash','total_count')\n",
    "\n",
    "# read txt labels file into dataframe\n",
    "df_train_Y = spark.read.load(LABELS, format=\"csv\", sep=\" \", inferSchema=\"true\", header=\"false\").toDF('Y')\n",
    "\n",
    "\n",
    "# combine features and labels into one dataframe\n",
    "# first, create a row index list w\n",
    "w = Window.orderBy(lit(1))\n",
    "\n",
    "# add row indexs to dataframes X and Y\n",
    "df_X=df_train_X.withColumn(\"rn\",row_number().over(w)-1)\n",
    "df_Y=df_train_Y.withColumn(\"rn\",row_number().over(w)-1)\n",
    "\n",
    "# join X dataframe and Y dataframe; and drop the row index\n",
    "df_train = df_X.join(df_Y,[\"rn\"]).drop(\"rn\")\n",
    "\n",
    "### if one would like to view the dataframe, please uncomment below line.\n",
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gmdqGhkvWHQo"
   },
   "outputs": [],
   "source": [
    "# split dataframes by the labels (Y=1, 2, 3, ..., 9)\n",
    "df_train.write.partitionBy(\"Y\").saveAsTable(\"dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y7FC2ptUWqTm"
   },
   "outputs": [],
   "source": [
    "# read splitted dataframes for each class (from 1 to 9)\n",
    "df1 = spark.read.parquet(\"spark-warehouse/dataframes/Y=1/*.parquet\")\n",
    "df2 = spark.read.parquet(\"spark-warehouse/dataframes/Y=2/*.parquet\")\n",
    "df3 = spark.read.parquet(\"spark-warehouse/dataframes/Y=3/*.parquet\")\n",
    "df4 = spark.read.parquet(\"spark-warehouse/dataframes/Y=4/*.parquet\")\n",
    "df5 = spark.read.parquet(\"spark-warehouse/dataframes/Y=5/*.parquet\")\n",
    "df6 = spark.read.parquet(\"spark-warehouse/dataframes/Y=6/*.parquet\")\n",
    "df7 = spark.read.parquet(\"spark-warehouse/dataframes/Y=7/*.parquet\")\n",
    "df8 = spark.read.parquet(\"spark-warehouse/dataframes/Y=8/*.parquet\")\n",
    "df9 = spark.read.parquet(\"spark-warehouse/dataframes/Y=9/*.parquet\")\n",
    "\n",
    "# calculate P(yk).\n",
    "pc1 = df1.count()/df_train.count()\n",
    "pc2 = df2.count()/df_train.count()\n",
    "pc3 = df3.count()/df_train.count()\n",
    "pc4 = df4.count()/df_train.count()\n",
    "pc5 = df5.count()/df_train.count()\n",
    "pc6 = df6.count()/df_train.count()\n",
    "pc7 = df7.count()/df_train.count()\n",
    "pc8 = df8.count()/df_train.count()\n",
    "pc9 = df9.count()/df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9l2GE8orauLR"
   },
   "outputs": [],
   "source": [
    "# creates empty lists for storing conditional probabilities for each class. (e.g. p1 for class 1)\n",
    "# each list has 257 elements, relating to 257 words in \"vocabulary\", in the order of: 00,01,02,...,??\n",
    "class1 = []\n",
    "class2 = []\n",
    "class3 = []\n",
    "class4 = []\n",
    "class5 = []\n",
    "class6 = []\n",
    "class7 = []\n",
    "class8 = []\n",
    "class9 = []\n",
    "\n",
    "# iteration from \"00\" to \"??\", totoally 257 interations\n",
    "for word in df1.schema.names:\n",
    "  # calculate total count of one word among all documents\n",
    "  total = df_train.agg(sum(word)).collect()[0][0]\n",
    "\n",
    "  # append probabilities to each class. count of each word in the documents belong to the specific class, divided by total count of this word among all documents.\n",
    "  class1.append(df1.agg(sum(word)).collect()[0][0]/total)\n",
    "  class2.append(df2.agg(sum(word)).collect()[0][0]/total)\n",
    "  class3.append(df3.agg(sum(word)).collect()[0][0]/total)\n",
    "  class4.append(df4.agg(sum(word)).collect()[0][0]/total)\n",
    "  class5.append(df5.agg(sum(word)).collect()[0][0]/total)\n",
    "  class6.append(df6.agg(sum(word)).collect()[0][0]/total)\n",
    "  class7.append(df7.agg(sum(word)).collect()[0][0]/total)\n",
    "  class8.append(df8.agg(sum(word)).collect()[0][0]/total)\n",
    "  class9.append(df9.agg(sum(word)).collect()[0][0]/total)\n",
    "  \n",
    "  print('iteration(',df1.schema.names.index(word)+1,'of 257 )')\n",
    "# should run around 8 minutes on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeridATrm2Hr"
   },
   "outputs": [],
   "source": [
    "# read test features.csv into dataframe.\n",
    "df_test_X = spark.read.options(header = \"true\", inferschema = \"true\").csv(TEST_PATH)\n",
    "\n",
    "# drop columns that are relitive values and the index column and \"hash\" column\n",
    "columns = []\n",
    "for name in df_test_X.schema.names:\n",
    "  if 'rel' in name:\n",
    "    columns.append(name)\n",
    "df_test_X = df_test_X.drop(*columns,'_c0','hash','total_count')\n",
    "\n",
    "# df_test_X.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm9M4mwHsIqr"
   },
   "outputs": [],
   "source": [
    "# parameter for reducing final probability scale\n",
    "par = 200\n",
    "\n",
    "# predicting test dataset\n",
    "# empty list to store predicted labels for all virus\n",
    "labels_predicted = []\n",
    "\n",
    "\n",
    "# iterations for each coloumn(word) of testing dataset (# of testing data)\n",
    "# for r in range(df_test_X.count()):\n",
    "for r in range(20):\n",
    "\n",
    "  print('iteration:',r,'of',range(df_test_X.count()))\n",
    "\n",
    "  # tem_list to store a row(iteration) of (count of words) from test dataset\n",
    "  tem_list = []\n",
    "  # to store conditional probabilities for each class.\n",
    "  # e.g. tem_list1 will contains 257 values, indicating 257 conditional probabilities for each word.\n",
    "  tem_list1 = []\n",
    "  tem_list2 = []\n",
    "  tem_list3 = []\n",
    "  tem_list4 = []\n",
    "  tem_list5 = []\n",
    "  tem_list6 = []\n",
    "  tem_list7 = []\n",
    "  tem_list8 = []\n",
    "  tem_list9 = []\n",
    "\n",
    "  # list of 9 probabilities predicted for every class.\n",
    "  prob = [0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "  # iteration of each word(column) in the present row(virus).\n",
    "  for c in range(len(df1.columns)):\n",
    "    # stores a row(iteration) of (count of words) from test dataset\n",
    "    tem_list.append(df_test_X.collect()[r][c]/par)\n",
    "    \n",
    "  \n",
    "  # to store conditional probabilities for each class.\n",
    "  tem_list1 = list(map(lambda x,y: x*y, class1,tem_list))\n",
    "  tem_list2 = list(map(lambda x,y: x*y, class2,tem_list))\n",
    "  tem_list3 = list(map(lambda x,y: x*y, class3,tem_list))\n",
    "  tem_list4 = list(map(lambda x,y: x*y, class4,tem_list))\n",
    "  tem_list5 = list(map(lambda x,y: x*y, class5,tem_list))\n",
    "  tem_list6 = list(map(lambda x,y: x*y, class6,tem_list))\n",
    "  tem_list7 = list(map(lambda x,y: x*y, class7,tem_list))\n",
    "  tem_list8 = list(map(lambda x,y: x*y, class8,tem_list))\n",
    "  tem_list9 = list(map(lambda x,y: x*y, class9,tem_list))\n",
    "\n",
    "  # multiply conditional probabilities together, then multiply by frequency of each class in training set. finally to get a probability for each class.\n",
    "  prob[0] = reduce(lambda x, y: x*y, tem_list1)*pc1\n",
    "  prob[1] = reduce(lambda x, y: x*y, tem_list2)*pc2\n",
    "  prob[2] = reduce(lambda x, y: x*y, tem_list3)*pc3\n",
    "  prob[3] = reduce(lambda x, y: x*y, tem_list4)*pc4\n",
    "  prob[4] = reduce(lambda x, y: x*y, tem_list5)*pc5\n",
    "  prob[5] = reduce(lambda x, y: x*y, tem_list6)*pc6\n",
    "  prob[6] = reduce(lambda x, y: x*y, tem_list7)*pc7\n",
    "  prob[7] = reduce(lambda x, y: x*y, tem_list8)*pc8\n",
    "  prob[8] = reduce(lambda x, y: x*y, tem_list9)*pc9\n",
    "  print(prob)\n",
    "  # get the class with max probability\n",
    "  maximum = 0\n",
    "  for p in prob:\n",
    "    if p >= maximum:\n",
    "      maximum = p\n",
    "\n",
    "  # append predicted class to a list\n",
    "  labels_predicted.append(prob.index(maximum)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "vlnGq1cDmHEH"
   },
   "outputs": [],
   "source": [
    "# write predicted labels into txt file from the list we finished from above line\n",
    "with open(\"files.txt\", \"w\") as output:\n",
    "  for row in range(df_test_X.count()-1):\n",
    "    output.write(str(int(labels_predicted[row])) + '\\n')\n",
    "  output.write(str(int(labels_predicted[row])))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
