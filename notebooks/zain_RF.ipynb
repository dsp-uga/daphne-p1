{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNVKTdMFYa0h"
   },
   "outputs": [],
   "source": [
    "# parameters for random forest\n",
    "src_bucket='gs://micky-practicum/'\n",
    "dest='home/zainmeekail/rf_predictions.txt'\n",
    "indicator_path='gs://uga-dsp/project1/files/'\n",
    "RANDOM_SEED = 13579\n",
    "RF_NUM_TREES = 300\n",
    "RF_MAX_DEPTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM2Kp9jXZsPD"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, FloatType, ArrayType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashes=sc.textFile(f'{indicator_path}X_train.txt').collect()\n",
    "train_labels=sc.textFile(f'{indicator_path}y_train.txt').collect()\n",
    "hash_labels=sc.broadcast({train_hashes[i]:train_labels[i] for i in range(len(train_hashes))})\n",
    "del train_hashes\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSchema(mode):\n",
    "    t=None\n",
    "    if mode=='int':\n",
    "        t=IntegerType\n",
    "    elif mode=='long':\n",
    "        t=LongType\n",
    "    elif mode=='float':\n",
    "        t=FloatType\n",
    "    schema = StructType([StructField('Y',LongType())]+[StructField(hexGen(i),t()) for i in range(257)])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_to_label(x):\n",
    "    out=[int(hash_labels.value[x[0]])]\n",
    "    out+=x[1:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to get the two-letter hex-string corresponding to a word_index\n",
    "# @param i: int in [0,256] mapped to corresponding hex in [00,...,FF,??]\n",
    "def hexGen(i):\n",
    "    return ('0'+str(hex(i)).upper()[2:])[-2:] if i <256 else '??'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "_data= sqlContext.read.load(f'{src_bucket}counts/X_train.parquet').rdd\\\n",
    "                    .map(hash_to_label)\n",
    "\n",
    "df=spark.createDataFrame(_data,schema=buildSchema('long'))\n",
    "\n",
    "assembler = VectorAssembler().setInputCols(df.schema.names[1:]).setOutputCol('features')\n",
    "\n",
    "data=assembler.transform(df).select('Y','features').selectExpr(\"Y as label\",'features')\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "trainingData=data\n",
    "#testData=\n",
    "_testData= sqlContext.read.load(f'{src_bucket}counts/X_test.parquet')\n",
    "\n",
    "test_assembler = VectorAssembler().setInputCols(_testData.schema.names[1:]).setOutputCol('features')\n",
    "\n",
    "testData=test_assembler.transform(_testData).select('hash','features')\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=RF_NUM_TREES,maxDepth=RF_MAX_DEPTH)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "output=predictions.select(\"hash\",\"predictedLabel\" ).rdd.collectAsMap()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hashes=sc.textFile(f'{indicator_path}X_test.txt').collect()\n",
    "with open(dest, 'a') as the_file:\n",
    "    for h in test_hashes:\n",
    "        the_file.write(f'{output[h]}\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Random Forest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
